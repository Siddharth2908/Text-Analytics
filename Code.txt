# Set working directory to ensure the sessions are saved
setwd("C:/Siddhartha/Personal Documents/BOK/Data Science/Tuturial")
install.packages("e1071")
install.packages("caret")
install.packages("quanteda")
install.packages("irlba")
install.packages("randomForest")
install.packages("ggplot2")
#Load source file into the session
Sourcedata<-read.csv("Spam.csv")
View(Sourcedata)
#Remove unwanted columns
Sourcedata<-Sourcedata[,1:2]
View(Sourcedata)
#Rename the columns 
names(Sourcedata)<-c("Category","Messages")
View(Sourcedata)
#Check data to see if there are any missing values
length(which(!complete.cases(Sourcedata)))
#Convert the data class as a "factor" which is typically the output that we would want to predict
Sourcedata$Category<-as.factor(Sourcedata$Category)
#Convert the data class as a "Character Vector" which is typically used to analyse
Sourcedata$Messages<-as.character(Sourcedata$Messages)
#Add a column which will have the length of the text
Sourcedata$TextLength<-nchar(Sourcedata$Messages)
#Explore the data to understand the spread
# 1. Number of possible outcome of a prediction inline with the categories 2.Feel of the distribution of the text data 3. Visualise the spread through loading ggplot2
prop.table(table(Sourcedata$Category))
summary(Sourcedata$TextLength)
View(Sourcedata)
library("ggplot2")
ggplot(Sourcedata, aes(x = TextLength, fill = Category))+
  theme_bw()+
  geom_histogram(binwidth = 5)+
  labs(y="Text Count", X = "Length of Text",title = "Data Visualization")
#Before every text analytics, split the data into 1. Training Set,2.Validation Set and 3. Test set or a minimum of 2 - Training and Test.
# To do this, load Caret package and then split the data. The training data will be used for creating a model and test will be to check whether the model works.
#This will be a 70%-30% split
#Set the random seed for data reproducability during every re-run. This is important as otherwise, the 70%-30% split will vary
library(caret)
set.seed(32984)
indexes<-createDataPartition(Sourcedata$Category, times=1, p=0.7, list = FALSE)
Train<-Sourcedata[indexes,]
Test<-Sourcedata[-indexes,]
prop.table(table(Train$Category))
prop.table(table(Test$Category))
#Create data pipelines to create and train the model. Perform Data exploration, Pre-processing and wrangling
#Load quanteda 
#Tokenize the text, remove stopwords, symbols, punctuations, hyphens, numbers. These are based on the context of the text under analysis
#Change everyt term/token into a lower case and stem them to remove same words with various tenses
library("quanteda")
Train.token<-tokens(Train$Messages, what = "word",remove_numbers = TRUE,remove_punct = TRUE, remove_symbols = TRUE, remove_hyphens = TRUE)
Train.token<-tokens_tolower(Train.token)
Train.token<-tokens_select(Train.token, stopwords(), selection = "remove")
Train.token<-tokens_wordstem(Train.token, language = "english")
#Convert the data into a class for Document Frequency Matrix (DFM)
Train.DFM<-dfm(Train.token, tolower = FALSE, remove = stopwords())
#Create DFM
Train.Matrix<-as.matrix(Train.DFM)
View(Train.Matrix)
#Add new column to the created matrix for "Category"
Train.Matrix<-cbind(Category = Train$Category, data.frame(Train.DFM))
#Clean up unwanted or unrecognized terms in DFM 
#Load caret
library(caret)
names(Train.Matrix)<-make.names(names(Train.Matrix))
# Use Cross-Validation(CV) as the basic model
# First step to create CV based model is to create stratified folds for 10 fold cross validation with 3 repetitions. This is more a best prax approach
# Set seed to ensure reproducability
set.seed(48743)
CV.folds<-createMultiFolds(Train$Category, k=10, times=5)
cv.control<-trainControl(method = "repeatedcv", number = 10, repeats = 3, index = CV.folds)
#second step is to create a model. To do this, install "doSNOW" package to run in parallel using multi cores as the CV takes long time. Create clusters to run model in parallel
library(doSNOW)
cl<-makeCluster(3,type="SOCK")
registerDoSNOW(cl)
#Create first CV model using Single Decision Tree "rpart"
Model1.rpart<-train(Category ~ ., data = Train.Matrix, method = "rpart", trControl = cv.control, tuneLength = 7)
stopCluster(cl)
#Check the results 
Model1.rpart
#The accuracy of the model can be improved by converting the matrix into TF-IDF based matrix. For this the first step is to create TF, IDF and TF.IDF function. 
#The using the created function, calculate, TF value, IDF value and TF*IDF values for every term in the DFM